========
SDNN编译
========

概述
====

``sdnn_build`` 工具实现模型编译，将不同框架的模型文件转换成SDNN框架统一的文件格式，该模型文件可以送到NPU仿真器进行模型的性能和精度评估，也可以拷贝到目标设备端，通过调用相应的runtime库，实现不同OS的部署。

.. image:: ../_static/images/devp/sdnn_build_framework.png


使用步骤
========

获取工具
--------

可以通过 `客户支持系统 <https://www.semidrive.com/>`_ 获取SDNN编译工具包,或者点击下表版本链接获取工具：

+---------------+------------------------------+
| 版本          | 说明                         |
+===============+==============================+
| `sdnn_build`_ | 该版本主要优化了模型文件体积 |
+---------------+------------------------------+

.. _sdnn_build: https://gitee.com/zgh551/sdnn_doc/releases/download/2.2.3/sdnn_cl-2.2.3-cp36-cp36m-linux_x86_64.whl

安装工具
--------

然后执行下述安装操作：

.. code-block:: shell
   :linenos:

   $ pip install sdnn_cl-2.2.3-cp36-cp36m-linux_x86_64.whl

.. note::
   #. 该工具需安装在 **章节2** 描述的docker环境中，其它环境可能会因为缺乏编译工具链而无法使用；
   #. 如果docker容器中存在版本一致sdnn-cl工具包，需要先卸载再安装；
   #. 如果docker容器中已经部署过TVM源码，则需要创建一个新的容器环境，不然会存在冲突；

模型编译
--------

模型编译目前支持两种模式，模型文件模式和json文件模式。

**模型文件模式**

.. code-block:: shell
   :linenos:

   $ sdnn_build -f mobilenet_v2.onnx -a slimai


**json文件模式**

.. code-block:: shell
   :linenos:

   $ sdnn_build -f mobilenet_v2.json -a slimai

参数说明
========

概述
----

编译时所用到的参数，大致可以分为一下几大类：

#. :ref:`base_params`：包含编译模型所需的基本编译选项，一般模型编译只需要关注该章节参数即可；
#. :ref:`advanced_params`：包含一些特殊情况下才需使用的参数选项；
#. :ref:`quant_params`：包含模型量化需要使用的参数；
#. :ref:`debug_params`：包含编译调试和模型仿真分析等相关参数；
#. :ref:`preprocess_params`：包含模型预处理相关参数，如果使用sdnn_test程序评估模型，需要关注该参数；
#. :ref:`postprocess_params`：包含模型后处理相关参数，如果使用sdnn_test程序评估模型，需要关注该参数；

基础参数
--------

.. table:: 基础参数
   :name: base_params

   +---------------+----------+---------+-----------------------------------------+--------------------+
   | 命令参数      | 缩略参数 | 默认值  | 可选范围                                | 说明               |
   +===============+==========+=========+=========================================+====================+
   | --help        | -h       |         |                                         | sdnn工具参数说明   |
   +---------------+----------+---------+-----------------------------------------+--------------------+
   | --version     | -v       |         |                                         | 查看SDNN版本       |
   +---------------+----------+---------+-----------------------------------------+--------------------+
   | --file        | -f       |         |                                         | 模型文件或json文件 |
   +---------------+----------+---------+-----------------------------------------+--------------------+
   | --cfg         | -c       |         |                                         | 指定配置文件路径   |
   +---------------+----------+---------+-----------------------------------------+--------------------+
   | --host        |          | aarch64 | x86_64, aarch64                         | 主机平台           |
   +---------------+----------+---------+-----------------------------------------+--------------------+
   | --os          |          | linux   | linux, android, qnx                     | 操作系统           |
   +---------------+----------+---------+-----------------------------------------+--------------------+
   | --accelerator | -a       | cpu     | cpu, gpu, slimai                        | 推理加速器         |
   +---------------+----------+---------+-----------------------------------------+--------------------+
   | --save        | -s       | models  |                                         | 模型库保存路径     |
   +---------------+----------+---------+-----------------------------------------+--------------------+
   | --name        | -n       | default |                                         | 模型别名           |
   +---------------+----------+---------+-----------------------------------------+--------------------+
   | --type        | -t       | onnx    | onnx, caffe, tf, tflite                 | 模型文件类型       |
   +---------------+----------+---------+-----------------------------------------+--------------------+
   | --domain      |          |         | Classification, Segmentation, Detection | 模型领域           |
   +---------------+----------+---------+-----------------------------------------+--------------------+

文件参数
^^^^^^^^

模型相关的文件都通过 ``-f`` 或 ``--file`` 参数指定，如果模型包含多个文件(caffe)，或者需要同时编译多个模型，可以通过指定多个 ``-f`` 或 ``--file`` 参数实现。

.. note::

   #. 如果指定的是模型文件，按需指定模型的相关参数，编译时会自动生成该模型对应的json配置文件，并将命令行中指定的参数保存到配置文件中，方便后续编译使用json配置文件;
   #. 如果指定的是json配置文件，该配置文件中包含了模型相关的参数信息，如果模型参数信息需要修改，可以打开文件修改，或者在编译时添加需要更改的参数来修正模型参数。

.. tabs::

   .. tab:: 模型文件

      **单模型编译**

      1. *单文件模型*

      例如onnx模型：

      .. code-block:: bash

         $ sdnn_build -f ./mobilenet_v2.onnx


      2. *多文件模型*

      例如caffe模型，包含两个文件：

      .. code-block:: bash

         $ sdnn_build -f ./mobilenet_v1.caffemodel -f ./mobilenet_v1.prototxt


      .. note::

         - 文件先后无限制


      **多模型编译**

      通过 ``--file`` 或 ``-f`` 参数指定多个模型的路径。

      .. code-block:: bash

         $ sdnn_build -f ./mobilenet_v2.onnx -f ./mobilenet_v1.caffemodel -f ./mobilenet_v1.prototxt


   .. tab:: JSON文件

      **单模型编译**

      .. code-block:: bash

         sdnn_build -f model1.json

      **多模型编译**

      .. code-block:: bash

         sdnn_build -f model1.json -f model2.json -f model3.json

      .. hint::

         json配置文件的完整格式如下：

         .. code-block:: json
            :linenos:

            {
             "model": {
                 "path": [
                     "xxx.caffemodel",
                     "xxx.prototxt"
                 ],
                 "name": "xxx",
                 "type": "xxx",
                 "channel_order": "RGB",
                 "domain": "Classification",
                 "output_layout": "NHWC",
                 "mean": [
                     0.485,
                     0.456,
                     0.406
                 ],
                 "std": [
                     0.229,
                     0.224,
                     0.225
                 ]
             },
             "dataset": {
                 "name": "ImageNet"
             },
             "metric": {
                 "method": "TopK",
                 "params": [
                     5
                 ]
             },
             "quant": {
                 "bit": "8bit"
             },
             "cfg": "./xxx.cfg"
            }


.. attention::

   #. 第一次执行模型文件编译后，会自动生成与该模型对应的 **cfg.json** 配置文件，当然也可以手动创建，按照上述完整格式填充必要信息；
   #. json配置文件中的一些参数，如果在编译时没有指定，会填充默认参数，使用自动生成的 **cfg.json** 文件前，请确认文件内容是否与模型匹配；
   #. 如果json文件中指定 **cfg字段** ，则会忽略 **quant** 和 **model** 字段中的参数，优先采用指定的配置文件进行NPU量化；
   #. 如果json文件中 ``无`` **cfg字段** ，则会根据 **quant** 和 **model** 字段中的参数,自动生成对应NPU的配置文件，文件后缀 **.autogen.cfg** ，文件路径与模型文件同目录。

CFG配置文件
^^^^^^^^^^^

通过参数 ``--cfg`` 或 ``-c`` 指定NPU量化配置文件路径，目前NPU设备指SlimAI。

主机平台
^^^^^^^^

通过参数 ``--host`` 配置模型编译的目标主机平台，其默认值为 **aarch64** 。

.. note::

   #. **x86_64**：指所有以x86_64架构CPU作为主机的设备，例如大多数PC和服务器，该格式的模型文件可以方便进行应用程序的调试；
   #. **aarch64**：指所有ARMV8架构的CPU，9系列芯片都属于该框架；



操作系统
^^^^^^^^

通过参数 ``--os`` 配置模型文件(.so)对应部署的操作系统，其默认值为 **linux** 。

.. note::

   #. 目前9系列芯片支持部署系统包括: linux、android和qnx；
   #. 如果主机选择x86_64，目前只支持linux系统；

加速设备
^^^^^^^^

通过参数 ``--accelerator`` 或 ``-a`` 配置模型编译目标部署的推理设备，其默认值为 **cpu** 。

.. note::

   #. 目前9系列芯片支持的加速设备包括：CPU、GPU和SlimAI。
   #. 如果主机选择x86_64，目前只支持CPU设备进行模型推理。

模型保存路径
^^^^^^^^^^^^

通过参数 ``--save`` 或 ``-s`` 配置模型文件的生成路径，其默认值为 **models** 。该路径下会生成以主机平台和操作系统组合的文件夹，在该文件夹内包含一个so库文件和一个.deploy.json文件。


模型别名
^^^^^^^^

通过参数 ``--name`` 或 ``-n`` 配置，如果不指定，会使用模型文件的名字作为输出模型库的别名。

.. note::

   #. 别名中不要包含 ``-`` 等字符；
   #. 如果模型文件名中包含 ``-`` 字符，且没有指定模型别名，则会自动将模型名中的 ``-`` 字符转成 ``_`` 字符。

模型文件类型
^^^^^^^^^^^^

通过参数 ``--type`` 或 ``-t`` 配置，如果不指定，会通过模型文件的后缀识别模型类型。

.. note::

   目前支持的模型有：``onnx`` 、``caffe`` 、``tf`` 、``tflite``

模型所属领域
^^^^^^^^^^^^

通过参数 ``--domain`` 配置，目前支持 **分类** 、 **检测** 和 **分割** 三种领域的模型;

进阶参数
--------

.. table:: 进阶参数
   :name: advanced_params

   +-----------------+----------+----------+-----------------+---------------------+
   | 命令参数        | 缩略参数 | 默认值   | 可选范围        | 说明                |
   +=================+==========+==========+=================+=====================+
   | --opt_level     | -l       | 3        | 1,2,3,4         | IR优化等级          |
   +-----------------+----------+----------+-----------------+---------------------+
   | --elf_mode      | -m       | separate | merge, separate | 选择模型合并模式    |
   +-----------------+----------+----------+-----------------+---------------------+
   | --elf_build_off | -b       | False    |                 | 使能关闭编译elf文件 |
   +-----------------+----------+----------+-----------------+---------------------+

IR优化等级
^^^^^^^^^^^

通过参数 ``--opt_level`` 或 ``-l`` 配置，其默认值为 **3** 。

.. note::

   如果出现优化后的算子不支持，可以适当调低；

ELF组合模式
^^^^^^^^^^^

通过参数 ``--elf_mode`` 或 ``-m`` 配置 **elf文件** 与 **so文件** 的组合模式，该参数只对SlimAI设备模型编译有效，其默认值为 **separate** 。

两种模式的区别如下图所示：

.. image:: ../_static/images/devp/elf_mode.png

.. tabs::

   .. tab:: separate

      模型编译时默认采用 **separate** 模式，该模式下，模型的 **so** 文件与 **elf** 文件独立生成。 其中，**elf** 文件包含所有模型的量化参数，**so** 文件只要包含对应模型的网络结构。

      .. note::

         如果是多模型编译，该参数的设置无效，强制为 **separate** 模式, 且会生成多个模型的 **so** 文件和单个 **elf** 文件。

   .. tab:: merge

      只有单模型编译支持 **merge** 模式，该模式下，将模型的 **elf** 文件集成进模型的 **so** 文件中，最终编译只输出单个 **so** 文件。

      .. note::

         该模式的存在主要方便快速测试，最终产品部署，建议采用 **separate** 模式。

模型部署时，需要手动拷贝 **elf** 文件到目标板指定目录下：

- **linux** ： ``/lib/firmware``
- **android** ： ``/vendor/firmware``
- **qnx** : ``/lib/firmware``

.. warning::

   qnx系统部署、多进程开发和Android系统APK代码开发都需要使用 **separate** 模式；

ELF文件生成
^^^^^^^^^^^

通过参数 ``--elf_build_off`` 或 ``-b`` 配置是否关闭 **elf文件** 编译过程，该参数只对SlimAI设备模型编译有效，其默认值为 **False** 。

.. note::

   #. 该参数用于控制 **elf文件** 是否重新生成，即控制是否对浮点模型进行量化操作；
   #. 不添加该参数，会基于浮点模型重新量化生成新的 **elf文件** ；
   #. 添加该参数则失效 **elf文件** 生成，一般使用场景是当模型第一次编译已经生成 **elf文件** 后，如果想生成其它 ``OS`` 的部署 **so文件** ，可以关闭生成elf文件，可以减少编译时间。

量化参数
--------

.. table:: 量化参数
   :name: quant_params

   +-------------+----------+--------+-------------------+--------------------+
   | 命令参数    | 缩略参数 | 默认值 | 可选范围          | 说明               |
   +=============+==========+========+===================+====================+
   | --quant_bit | -qb      |        | 8bit, 16bit, auto | 配置模型的量化位宽 |
   +-------------+----------+--------+-------------------+--------------------+


量化位宽
^^^^^^^^

通过参数 ``--quant_bit`` 或 ``-qb`` 配置模型量化位宽，目前可选 **8bit** 、**16bit** 和 **auto** 。

.. note::
   - auto模式还未支持，后续增加该功能；

仿真调试参数
------------

.. table:: 仿真调试参数
   :name: debug_params

   +-------------------+----------+--------+----------+----------------------+
   | 命令参数          | 缩略参数 | 默认值 | 可选范围 | 说明                 |
   +===================+==========+========+==========+======================+
   | --debug           | -d       | False  |          | 打印编译调试信息     |
   +-------------------+----------+--------+----------+----------------------+
   | --emu             | -e       | False  |          | 使能生成仿真模式文件 |
   +-------------------+----------+--------+----------+----------------------+
   | --dump_ir         | -ir      | False  |          | dump IR文件          |
   +-------------------+----------+--------+----------+----------------------+
   | --dump_quant_err  | -qe      | False  |          | dump 每层相似度      |
   +-------------------+----------+--------+----------+----------------------+
   | --dump_layer_prof | -lp      | False  |          | dump 每层性能信息    |
   +-------------------+----------+--------+----------+----------------------+
   | --dump_path       | -p       | dump   |          | dump 文件目录        |
   +-------------------+----------+--------+----------+----------------------+


Debug模式
^^^^^^^^^

通过参数 ``--debug`` 或 ``-d`` 配置，其默认值为 **False** 。

.. note::

   开启Debug模式编译，会输出编译阶段的中间信息，并保存相关调试信息，便于模型编译的调试。

仿真模型文件生成
^^^^^^^^^^^^^^^^

通过参数 ``--emu`` 或 ``-e`` 配置仿真模式的模型文件生成，其默认值为 **False** 。

.. note::

   仿真模型文件生成目前只有slimai加速设备支持


Dump Relay IR信息
^^^^^^^^^^^^^^^^^

通过参数 ``--dump_ir`` 或 ``-ir`` 配置生成模型的Relay IR信息文件，文件会保存在 ``--dump_path`` 参数指定的路径，其默认值为 **False** 。


Dump 量化误差信息
^^^^^^^^^^^^^^^^^

通过参数 ``--dump_quant_err`` 或 ``-qe`` 配置生成模型每层量化误差分析文件，文件会保存在 ``--dump_path`` 参数指定的路径，其默认值为 **False**。

量化相似度分析报告格式说明
""""""""""""""""""""""""""

相似度报告similarity.txt最终输出如下数据，即相同节点的量化前后的余弦相似度。

.. code-block:: bash
   :linenos:

   The similarity of nn_conv2d_760 between fixed and float model is: 0.996759
   The similarity of nn_bias_add_750 between fixed and float model is: 0.999363
   ...

上述余弦相似度的值是此节点在所有验证图片上的均值。和量化相似度分析报告同目录会一起有很多文件，
命名风格为：网络名称_数字， 网络名称_ref，如下：

.. code-block:: bash
   :linenos:

   ...
   mobilenet_v2_96
   mobilenet_v2_97
   mobilenet_v2_98
   mobilenet_v2_99
   ...
   mobilenet_v2_ref

其中网络名称_数字的文件夹如mobilenet_v2_97， 存放的是此网络在range参数为97时候的定点输出，网络名称_ref的文件夹包含的是此网络的浮点输出。每个文件夹中包含大量blob文件，如下：

.. code-block:: bash
   :linenos:

   cat_add_190.blob
   cat_add_200.blob
   cat_add_290.blob

上面示例中，cat代表的是图片的名字，add_*代表的是节点的名字，整个代表的是在输入图片为cat.png时候add_190、add_200、add_290节点的输出。进去blob中，第一行记录的是数据的缩放因子和维度以及数据格式。如下cat_add_190.blob中。

.. code-block:: bash
   :linenos:

   63.499031 7 160 7 1 S8
   ...

64.499031为缩放的因子，7 160 7 1为此节点输出的维度，S8为数据的类型。从第二行开始为实际数据。


Dump 模型每层性能信息
^^^^^^^^^^^^^^^^^^^^^

通过参数 ``--dump_layer_prof`` 或 ``-lp`` 配置生成模型的性能分析文件，文件会保存在 ``--dump_path`` 参数指定的路径，其默认值为 **False** 。

性能分析报告格式说明
""""""""""""""""""""

分析报告最终会输出如下数据，即网络的帧率：

.. code-block:: bash
   :linenos:

   PERFORMANCE 120.10 FPS @748.00MHz

根据DSP的时钟频率748MHZ，可以计算得出一个时钟周期为1.3369ns，则总的网络推理时间计算方式如下：

.. math::

   time_{Inference}= cycles_{total} * clock_{cycle}

同理，根据每层网络的时钟周期数可以计算每层网络的耗时，但需要注意的是，仿真输出的层名与原始模型的层名不一定能够匹配，仿真输出的层名，是多算子融合和优化的结果。

   +----------------------+-------------------------------------------+
   | 参数                 | 含义                                      |
   +======================+===========================================+
   | Total Cycles         | 该层网络总的时钟周期                      |
   +----------------------+-------------------------------------------+
   | XI Kernel Cycles     | 该层网络内核计算周期数                    |
   +----------------------+-------------------------------------------+
   | Edge Ext Cycles      | 该层网络边沿计算周期数                    |
   +----------------------+-------------------------------------------+
   | DSP Idle WAIT Cycles | 该层网络DSP空闲等待周期数                 |
   +----------------------+-------------------------------------------+
   | MACs per Cycles      | 该层网络每个时钟周期所使用的MAC单元数量   |
   +----------------------+-------------------------------------------+
   | MAC%                 | 该层网络的MAC单元利用率                   |
   +----------------------+-------------------------------------------+
   | MACs                 | 计算该层网络总共的MAC单元数量             |
   +----------------------+-------------------------------------------+
   | DMA Queue Size       | 该层网络使用的DMA队列数量                 |
   +----------------------+-------------------------------------------+
   | Layer Name           | 该层网络命名 [注：与原始模型的层名不对应] |
   +----------------------+-------------------------------------------+

Dump 路径
^^^^^^^^^

通过参数 ``--dump_path`` 或 ``-p`` 配置dump文件的保存路径，其默认值为 **dump** 。

预处理参数
----------

.. table:: 预处理参数
   :name: preprocess_params

   +-----------------+----------+--------+----------+--------------+
   | 命令参数        | 缩略参数 | 默认值 | 可选范围 | 说明         |
   +=================+==========+========+==========+==============+
   | --channel_order | -co      | RGB    | RGB, BGR | 颜色通道循序 |
   +-----------------+----------+--------+----------+--------------+
   | --mean          |          |        |          | 平均值       |
   +-----------------+----------+--------+----------+--------------+
   | --std           |          |        |          | 方差         |
   +-----------------+----------+--------+----------+--------------+
   | --dataset       | -ds      |        | ImageNet | 数据集       |
   +-----------------+----------+--------+----------+--------------+

模型输入通道次序
^^^^^^^^^^^^^^^^

通过参数 ``--channel_order`` 或 ``-co`` 配置模型输入通道格式，目前可选值为 **RGB** 或 **BGR** 。

模型输入平均值
^^^^^^^^^^^^^^

通过参数 ``--mean`` 配置，通道数值通过 ``,`` 字符分隔，中间不能有空格符号。

.. note::

   例如--mean 1.23,45.67,8.0

模型输入方差
^^^^^^^^^^^^

通过参数 ``--std`` 配置，通道数值通过 ``,`` 字符分隔，中间不能有空格符号。

.. note::

   例如--std 1.2,32.34,34.34


数据集
^^^^^^

通过参数 ``--dataset`` 或 ``-ds`` 配置模型的数据集类型，目前支持 **ImageNet** 数据集处理。


后处理参数
----------

.. table:: 后处理参数
   :name: postprocess_params

   +-----------------+----------+---------+---------------+------------------------+
   | 命令参数        | 缩略参数 | 默认值  | 可选范围      | 说明                   |
   +=================+==========+=========+===============+========================+
   | --output_layout | -ol      |         | NCHW, NHWC    | 设置模型输出节点layout |
   +-----------------+----------+---------+---------------+------------------------+
   | --metric        | -mt      | BinData | BinData, TopK | metric方法             |
   +-----------------+----------+---------+---------------+------------------------+
   | --metric_params | -mp      |         |               | metric参数             |
   +-----------------+----------+---------+---------------+------------------------+


输出通道布局
^^^^^^^^^^^^

通过参数 ``--output_layout`` 或 ``-ol`` 配置是否在模型输出节点添加transpose算子，进行输出节点的通道变换，其默认值为 **False** 。

.. note::

   #. 模型输出节点维度必须是4,才能使能该选项；
   #. 参数值表示的是通道目标布局，即原模型如果输出格式是NCHW，则设置参数-ol NHWC，则会在模型输出节点基础上添加tranpose算子，实现NCHW到NHWC的变换。

metric方法
^^^^^^^^^^

通过参数 ``--metric`` 或 ``-mt`` 配置模型的测量方法，其默认值为 **BinData** 。

metric 参数
^^^^^^^^^^^

通过参数 ``--metric_params`` 或 ``-mp`` 配置模型的测量方法的参数。

   +---------+-----------------+-------------------------+
   | 方法    | 参数            | 说明                    |
   +=========+=================+=========================+
   | BinData | 无              | 保存模型输出通道数据    |
   +---------+-----------------+-------------------------+
   | TopK    | 参数k: k1,k2,k3 | 计算前K个权重大值的索引 |
   +---------+-----------------+-------------------------+
